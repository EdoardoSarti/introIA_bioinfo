{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63bd6c3d",
   "metadata": {},
   "source": [
    "# Clusterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1383968e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T15:14:30.652382Z",
     "start_time": "2021-11-13T15:14:25.774122Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc15a068",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T15:14:38.398191Z",
     "start_time": "2021-11-13T15:14:30.660517Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6550ffa",
   "metadata": {},
   "source": [
    "## Learn about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee502a6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T15:14:38.545417Z",
     "start_time": "2021-11-13T15:14:38.449002Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "USArrests = pd.read_csv(\"data/USArrests.csv\", sep=\",\")\n",
    "USArrests['StateAbbrv'] = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\",\"DE\", \"FL\", \"GA\", \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\",\"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\",\"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]\n",
    "USArrests.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a87b8b",
   "metadata": {},
   "source": [
    "The data have more dimensions than we can easily visualize, so we use PCA to reduce the number of dimensions.\n",
    "\n",
    "As with clustering, it is necessary to scale the data before applying PCA.\n",
    "\n",
    "You note that we scale the entire dataset, rather than adjusting the scale on the train and carrying over this scaling to future data (i.e. the test). Since we will not be using a test set here, it is therefore correct to use all of the scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dfa588",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T15:14:51.275650Z",
     "start_time": "2021-11-13T15:14:41.274769Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "df = USArrests[['Murder','Assault','UrbanPop','Rape']]\n",
    "scaled_df = pd.DataFrame(preprocessing.scale(df), index=USArrests['State'], columns = df.columns)\n",
    "\n",
    "fitted_pca = PCA().fit(scaled_df)\n",
    "USArrests_pca = fitted_pca.transform(scaled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222e949c",
   "metadata": {},
   "source": [
    "The following `biplot function` plots the first two PCA components, and provides some helpful annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3ae0e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T15:14:53.227359Z",
     "start_time": "2021-11-13T15:14:52.717081Z"
    }
   },
   "outputs": [],
   "source": [
    "def biplot(pca_data, fitted_pca, original_dim_labels, point_labels, cluster=None):\n",
    "\n",
    "    pca1_scores = pca_data[:,0]\n",
    "    pca2_scores = pca_data[:,1]\n",
    "\n",
    "    # define the colormap\n",
    "    cmap = plt.cm.jet\n",
    "    # extract all colors from the .jet map\n",
    "    cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "    # create the new map\n",
    "    newmap = cmap.from_list('Custom cmap', cmaplist, cmap.N)\n",
    "            \n",
    "    # plot each point in 2D post-PCA space\n",
    "    if cluster is None:\n",
    "        cluster = [0]*len(pca1_scores)\n",
    "    else:\n",
    "        step = cmap.N//max(cluster)\n",
    "        cluster = [step*i for i in cluster]\n",
    "    plt.scatter(pca1_scores, pca2_scores, c=cluster, cmap=newmap)\n",
    "\n",
    "    # label each point\n",
    "    for i in range(len(point_labels)):\n",
    "        #print(\"pca1\", pca1_scores[i])\n",
    "        #print(\"pca2\", pca2_scores[i])\n",
    "        #print(\"label\", point_labels[i])\n",
    "        plt.text(pca1_scores[i],pca2_scores[i], point_labels[i])\n",
    "\n",
    "    #for each original dimension, plot what an increase of 1 in that dimension means in this space\n",
    "    for i in range(fitted_pca.components_.shape[1]):\n",
    "        raw_dims_delta_on_pca1 = fitted_pca.components_[0,i]\n",
    "        raw_dims_delta_on_pca2 = fitted_pca.components_[1,i]\n",
    "        plt.arrow(0, 0, raw_dims_delta_on_pca1, raw_dims_delta_on_pca2 ,color = 'r',alpha = 1)\n",
    "        plt.text(raw_dims_delta_on_pca1*1.1, raw_dims_delta_on_pca2*1.1, original_dim_labels[i], color = 'g', ha = 'center', va = 'center')\n",
    "\n",
    "plt.figure(figsize=(8.5,8.5))\n",
    "plt.xlim(-3.5,3.5)\n",
    "plt.ylim(-3.5,3.5)\n",
    "plt.xlabel(\"PC{}\".format(1))\n",
    "plt.ylabel(\"PC{}\".format(2))\n",
    "plt.grid()\n",
    "\n",
    "biplot(USArrests_pca, fitted_pca,\n",
    "       original_dim_labels=scaled_df.columns,\n",
    "       point_labels=USArrests['StateAbbrv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2d20e6",
   "metadata": {},
   "source": [
    "The red arrows and green text give us a sense of direction. If any state had 'murder' increase by one (scaled) unit, it would move in the direction of the 'murder' line by that amount.\n",
    "\n",
    "An increase by one (scaled) unit of both 'murder' and 'Urban Pop' would apply both moves.\n",
    "\n",
    "We can also make inferrences about what combination of crimes and population puts California at its observed point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644f07e2",
   "metadata": {},
   "source": [
    "## Distances\n",
    "\n",
    "One of the key ideas in clustering is the distance or disimilarity between points. Euclidean distance is common, though one is free to define domain-specific measures of how similar/distant two observations are.\n",
    "\n",
    "* scipy `pdist` function computes the distances between all pairs of data points (which can be quite expensive for large data).\n",
    "* scipy `squareform` turns the result into a numpy array (the raw format avoids storing redundant values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ed3585",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T15:14:58.853868Z",
     "start_time": "2021-11-13T15:14:58.847265Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "dist_eucl = pdist(scaled_df,metric=\"euclidean\")\n",
    "distances = pd.DataFrame(squareform(dist_eucl), index=USArrests[\"State\"].values, columns=USArrests[\"State\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edb0d7a",
   "metadata": {},
   "source": [
    "We can then easily print the matrix of distances between states using seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896c571b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T15:15:01.154674Z",
     "start_time": "2021-11-13T15:15:00.007392Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(11,8.5))\n",
    "sns.heatmap(distances)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbd3bcc",
   "metadata": {},
   "source": [
    "## Clustering with K-means\n",
    "\n",
    "Kmeans is a classical, workhorse clustering algorithm, and a common place to start. It assumes there are K centers and, starting from random guesses, algorithmically improves its guess about where the centers must be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4193c157",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Using the KMeans function of sklearn, propose a first clustering in 3 clusters with 25 runs\n",
    "\n",
    "To help you, you must use the parameters:\n",
    "    \n",
    "* `n_clusters`: The number of clusters to form as well as the number of centroids to generate.\n",
    "* `n_init`: Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia.\n",
    "* `random_state`: Determines random number generation for centroid initialization. Use an int to make the randomness deterministic.\n",
    "    \n",
    "Using the `cluster_centers` attribute, print the coodinate of each cluster centers.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f448bb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T15:19:57.346081Z",
     "start_time": "2021-11-13T15:19:57.279411Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "''' TO DO '''\n",
    "arrests_km = KMeans(n_clusters=...,\n",
    "                    n_init=...,\n",
    "                    random_state=...).fit(scaled_df)\n",
    "\n",
    "arrests_km.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352b6688",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T15:19:57.674584Z",
     "start_time": "2021-11-13T15:19:57.660322Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(arrests_km.cluster_centers_,columns=['Murder','Assault','UrbanPop','Rape'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10d7cae",
   "metadata": {},
   "source": [
    "The `labels_` attribute tell us which cluster each point was assigned to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11e3c2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T15:19:58.228533Z",
     "start_time": "2021-11-13T15:19:58.213934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Copy the original dataset\n",
    "df_cluster = scaled_df.copy()\n",
    "# Add label to the dataset\n",
    "df_cluster['Cluster'] = arrests_km.labels_\n",
    "# Print part of the dataset\n",
    "df_cluster['Cluster'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79444f8",
   "metadata": {},
   "source": [
    "We now calculate the average of the points of each cluster to obtain the centroid found by K-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9991fce2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T15:19:58.979798Z",
     "start_time": "2021-11-13T15:19:58.965679Z"
    }
   },
   "outputs": [],
   "source": [
    "df_centroids = df_cluster.groupby('Cluster').mean()\n",
    "df_centroids.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de1ec5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T15:12:18.376369Z",
     "start_time": "2021-11-13T15:12:18.366326Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Do we get the same value as when looking at the cluster_centers_ attribute ?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f177d3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T15:19:59.997885Z",
     "start_time": "2021-11-13T15:19:59.989356Z"
    }
   },
   "outputs": [],
   "source": [
    "arrests_km...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eed4066",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T16:53:55.979093Z",
     "start_time": "2021-11-12T16:53:55.962526Z"
    }
   },
   "source": [
    "Plot the dataset with the cluster attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf08319f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T15:20:01.236778Z",
     "start_time": "2021-11-13T15:20:00.878741Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8.5,8.5))\n",
    "plt.xlim(-3.5,3.5)\n",
    "plt.ylim(-3.5,3.5)\n",
    "plt.xlabel(\"PC{}\".format(1))\n",
    "plt.ylabel(\"PC{}\".format(2))\n",
    "plt.grid()\n",
    "\n",
    "biplot(USArrests_pca, fitted_pca,\n",
    "       original_dim_labels=scaled_df.columns,\n",
    "       point_labels=USArrests['StateAbbrv'],\n",
    "       cluster=arrests_km.labels_)\n",
    "\n",
    "Centroids_pca = fitted_pca.transform(arrests_km.cluster_centers_)\n",
    "\n",
    "biplot(Centroids_pca, fitted_pca,\n",
    "       original_dim_labels=scaled_df.columns,\n",
    "       point_labels=np.unique(arrests_km.labels_),\n",
    "       cluster=np.unique(arrests_km.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8176bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T16:56:18.642444Z",
     "start_time": "2021-11-12T16:56:18.638926Z"
    }
   },
   "source": [
    "### Plot the distortions of K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c890e44b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T17:00:44.589115Z",
     "start_time": "2021-11-12T17:00:44.581598Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "You can easily run K-Means with several run for a range of clusters using a for loop and collecting the distortions into a list.\n",
    "    \n",
    "You can collect the distortions using the `inertia_`attribute. `inertia_`is the sum of squared distances of samples to their closest cluster center, weighted by the sample weights if provided.\n",
    "    \n",
    "Then plots the distortions of K-means.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda063e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T15:18:54.365189Z",
     "start_time": "2021-11-13T15:18:53.887647Z"
    }
   },
   "outputs": [],
   "source": [
    "distortions = []\n",
    "for k in range(1,11):\n",
    "    km = KMeans(n_clusters=k,\n",
    "                init='random',\n",
    "                n_init=25,\n",
    "                random_state=123).fit(scaled_df)\n",
    "    distortions.append(km...)\n",
    "    \n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(range(1,11), distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a28a93",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "What is the best number of clusters when using the Elbow method?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37abcb60",
   "metadata": {},
   "source": [
    "### Silhouette Plots\n",
    "Silhouette plots give rich information on the quality of a clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4279b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T15:18:54.664495Z",
     "start_time": "2021-11-13T15:18:54.646323Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "#modified code from http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "\n",
    "def silplot(X, cluster_labels, clusterer, pointlabels=None):\n",
    "    n_clusters = clusterer.n_clusters\n",
    "\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(11,8.5)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters = \", n_clusters,\n",
    "          \", the average silhouette_score is \", silhouette_avg,\".\",sep=\"\")\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(0,n_clusters+1):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "\n",
    "    # axes will be first 2 PCA components\n",
    "    pca = PCA(n_components=2).fit(X)\n",
    "    X_pca = pca.transform(X)\n",
    "    ax2.scatter(X_pca[:, 0], X_pca[:, 1], marker='.', s=200, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "    xs = X_pca[:, 0]\n",
    "    ys = X_pca[:, 1]\n",
    "\n",
    "\n",
    "    if pointlabels is not None:\n",
    "        for i in range(len(xs)):\n",
    "            plt.text(xs[i],ys[i],pointlabels[i])\n",
    "\n",
    "    # Labeling the clusters (transform to PCA space for plotting)\n",
    "    centers = pca.transform(clusterer.cluster_centers_)\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % int(i), alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"PC1\")\n",
    "    ax2.set_ylabel(\"PC2\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39a393a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T15:18:55.373437Z",
     "start_time": "2021-11-13T15:18:54.857755Z"
    }
   },
   "outputs": [],
   "source": [
    "fitted_km = KMeans(n_clusters=3,n_init=25,random_state=123).fit(scaled_df)\n",
    "cluster_labels = fitted_km.labels_\n",
    "\n",
    "silplot(scaled_df.values, cluster_labels, fitted_km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6389f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T15:18:55.400991Z",
     "start_time": "2021-11-13T15:18:55.377532Z"
    }
   },
   "outputs": [],
   "source": [
    "# Objects with negative silhouette\n",
    "sil = silhouette_samples(scaled_df, fitted_km.labels_)\n",
    "\n",
    "USArrests.loc[sil<=0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0615e2ce",
   "metadata": {},
   "source": [
    "### Silhouette Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2900cd9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T17:12:26.475324Z",
     "start_time": "2021-11-12T17:12:26.467696Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Now plot the score of the silhouette score (cf. <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html\">sklearn.metrics.silhouette_score</a>) as the number of clusters varies.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7975445",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T15:18:56.818670Z",
     "start_time": "2021-11-13T15:18:56.210885Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "scores = [0]  # silhouette score for 1 cluster\n",
    "for k in range(2,11):\n",
    "    km = KMeans(n_clusters=...,\n",
    "                init=...,\n",
    "                n_init=...,\n",
    "                random_state=...).fit(scaled_df)\n",
    "    # Calculate silouette_score\n",
    "    sil = ...\n",
    "    scores.append(sil)\n",
    "\n",
    "print(\"Optimized at\", max(range(len(scores)), key=scores.__getitem__)+1, \"clusters\")\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(range(1,11), scores, 'bx-')\n",
    "plt.xlabel('Number of clusters $k$')\n",
    "plt.ylabel('Average Silhouette')\n",
    "plt.title('The Silhouette Method showing the optimal $k$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c999ea7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T17:12:34.169433Z",
     "start_time": "2021-11-12T17:12:34.155932Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "What is the best number of clusters when using the Silhouet method?\n",
    "\n",
    "Is it the same as with the Elbow method?\n",
    "    \n",
    "Cluster the dataset with the best number of clusters and try to explain each cluster.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fcb892",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T15:22:27.369676Z",
     "start_time": "2021-11-13T15:22:26.894326Z"
    }
   },
   "outputs": [],
   "source": [
    "''' TO DO '''\n",
    "arrests_km = KMeans(...).fit(scaled_df)\n",
    "\n",
    "plt.figure(figsize=(8.5,8.5))\n",
    "plt.xlim(-3.5,3.5)\n",
    "plt.ylim(-3.5,3.5)\n",
    "plt.xlabel(\"PC{}\".format(1))\n",
    "plt.ylabel(\"PC{}\".format(2))\n",
    "plt.grid()\n",
    "\n",
    "biplot(USArrests_pca, fitted_pca,\n",
    "       original_dim_labels=scaled_df.columns,\n",
    "       point_labels=USArrests['StateAbbrv'],\n",
    "       cluster=arrests_km.labels_)\n",
    "\n",
    "Centroids_pca = fitted_pca.transform(arrests_km.cluster_centers_)\n",
    "biplot(Centroids_pca, fitted_pca,\n",
    "       original_dim_labels=scaled_df.columns,\n",
    "       point_labels=np.unique(arrests_km.labels_),\n",
    "       cluster=np.unique(arrests_km.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2449348",
   "metadata": {},
   "source": [
    "#### Gap Statistic\n",
    "\n",
    "The gap statistic is a method for estimating the number of clusters in a dataset. The techniques uses the output of any clustering algorithm, comparing the change in within-cluster dispersion withthat expected under an appropriate reference null distribution. The original paper is [here](https://web.stanford.edu/~hastie/Papers/gap.pdf).\n",
    "\n",
    "* Githup page: [https://github.com/milesgranger/gap_statistic]\n",
    "* [K-Means Clustering and the Gap-Statistics](https://towardsdatascience.com/k-means-clustering-and-the-gap-statistics-4c5d414acd29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a88086d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T15:35:32.672869Z",
     "start_time": "2021-11-13T15:35:32.669417Z"
    }
   },
   "outputs": [],
   "source": [
    "# need to install library 'gap-stat'\n",
    "#!pip install gap-stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a54757",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T15:35:34.292336Z",
     "start_time": "2021-11-13T15:35:33.300687Z"
    }
   },
   "outputs": [],
   "source": [
    "from gap_statistic import OptimalK\n",
    "\n",
    "gs_obj = OptimalK()\n",
    "\n",
    "n_clusters = gs_obj(scaled_df.values,                    # The dataset\n",
    "                    n_refs=100,                           # Number of runs\n",
    "                    cluster_array=np.arange(1, 15))      # Range of clusterization\n",
    "print('Optimal clusters: ', n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb8f864",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T15:35:34.350495Z",
     "start_time": "2021-11-13T15:35:34.307155Z"
    }
   },
   "outputs": [],
   "source": [
    "gs_obj.gap_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d59587",
   "metadata": {},
   "source": [
    "The columns of the dataframe are:\n",
    "\n",
    "* n_clusters - The number of clusters for which the statistics in this row were calculated.\n",
    "* gap_value - The Gap value for this n.\n",
    "* gap* - The Gap* value for this n.\n",
    "* ref_dispersion_std - The standard deviation of the reference distributions for this n.\n",
    "* sk - The standard error of the Gap statistic for this n.\n",
    "* sk* - The standard error of the Gap* statistic for this n.\n",
    "* diff - The diff value for this n (see the methodology section for details).\n",
    "* diff* - The diff* value for this n (corresponding to the diff value for Gap*).\n",
    "\n",
    "Additionally, the relation between the above measures and the number of clusters can be plotted by calling the OptimalK.plot_results() method (meant to be used inside a Jupyter Notebook or a similar IPython-based notebook), which prints four plots:\n",
    "\n",
    "* A plot of the Gap value versus n, the number of clusters.\n",
    "* A plot of diff versus n.\n",
    "* A plot of the Gap* value versus n, the number of clusters.\n",
    "* A plot of the diff* value versus n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9000f0b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T15:35:35.283871Z",
     "start_time": "2021-11-13T15:35:34.353529Z"
    }
   },
   "outputs": [],
   "source": [
    "gs_obj.plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a556ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T15:34:33.210437Z",
     "start_time": "2021-11-13T15:34:33.202451Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">    \n",
    "Cluster the dataset with the best number of clusters givent by gap-stat and try to explain each cluster.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6426f47a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T15:35:35.788834Z",
     "start_time": "2021-11-13T15:35:35.290129Z"
    }
   },
   "outputs": [],
   "source": [
    "''' TO DO '''\n",
    "arrests_km = KMeans(...).fit(scaled_df)\n",
    "\n",
    "plt.figure(figsize=(8.5,8.5))\n",
    "plt.xlim(-3.5,3.5)\n",
    "plt.ylim(-3.5,3.5)\n",
    "plt.xlabel(\"PC{}\".format(1))\n",
    "plt.ylabel(\"PC{}\".format(2))\n",
    "plt.grid()\n",
    "\n",
    "biplot(USArrests_pca, fitted_pca,\n",
    "       original_dim_labels=scaled_df.columns,\n",
    "       point_labels=USArrests['StateAbbrv'],\n",
    "       cluster=arrests_km.labels_)\n",
    "\n",
    "Centroids_pca = fitted_pca.transform(arrests_km.cluster_centers_)\n",
    "biplot(Centroids_pca, fitted_pca,\n",
    "       original_dim_labels=scaled_df.columns,\n",
    "       point_labels=np.unique(arrests_km.labels_),\n",
    "       cluster=np.unique(arrests_km.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dd3746",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T15:33:09.636279Z",
     "start_time": "2021-11-13T15:33:09.629159Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">    \n",
    "Try to explain each cluster\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbb756d",
   "metadata": {},
   "source": [
    "## Hierarchical clustering\n",
    "\n",
    "K-means is a very 'hard' clustering: points belong to exactly one cluster, no matter what. A hierarchical clustering creates a nesting of clusters as existing clusters are merged or split.\n",
    "\n",
    "Dendograms (literally: branch graphs) can show the pattern of splits/merges. Unfortunately dendograms are not implemented in sklearn. We have to use the scipy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143249bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T17:58:07.376851Z",
     "start_time": "2021-11-13T17:58:07.372501Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as hac\n",
    "from scipy.spatial.distance import pdist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2c8757",
   "metadata": {},
   "source": [
    "The following code shows different dendograms build with different algorithms:\n",
    "\n",
    "1. Min (or single linkage) algorithm with euclidian distance: the distance of the new cluster, is the min.\n",
    "1. Max (or complete linkage) algorithm with euclidian distance: : the distance of the new cluster, is the max.\n",
    "1. Ward algorithm with euclidean distance: the distance of the new cluster, is the average of the sum of the square of the distance.\n",
    "\n",
    "![three linkage type](https://www.researchgate.net/profile/Pamela-Guevara/publication/281014334/figure/fig57/AS:418517879934980@1476793847581/The-three-linkage-types-of-hierarchical-clustering-single-link-complete-link-and.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3e9dda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T18:01:29.575608Z",
     "start_time": "2021-11-13T18:01:28.915189Z"
    }
   },
   "outputs": [],
   "source": [
    "# Min (or single linkage) algorithm with euclidian distance\n",
    "plt.figure(figsize=(11,8.5))\n",
    "dist_mat = pdist(scaled_df, metric=\"minkowski\", p=2) # Euclidian\n",
    "ward_data = hac.single(dist_mat) # min\n",
    "\n",
    "hac.dendrogram(ward_data, labels=USArrests[\"State\"].values);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce645418",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T18:01:30.194875Z",
     "start_time": "2021-11-13T18:01:29.579512Z"
    }
   },
   "outputs": [],
   "source": [
    "# Max (or complete linkage) algorithm with euclidian distance\n",
    "plt.figure(figsize=(11,8.5))\n",
    "dist_mat = pdist(scaled_df, metric=\"minkowski\", p=2) # Euclidian\n",
    "ward_data = hac.complete(dist_mat) # max\n",
    "\n",
    "hac.dendrogram(ward_data, labels=USArrests[\"State\"].values);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd2bce3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T18:01:31.450023Z",
     "start_time": "2021-11-13T18:01:30.197720Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ward algorithm with euclidean distance\n",
    "plt.figure(figsize=(11,8.5))\n",
    "dist_mat = pdist(scaled_df, metric=\"minkowski\", p=2) # Euclidian\n",
    "ward_data = hac.ward(dist_mat) # average\n",
    "\n",
    "hac.dendrogram(ward_data, labels=USArrests[\"State\"].values);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4e6505",
   "metadata": {},
   "source": [
    "If sklearn can't draw dendogrames directly. It is nevertheless possible :\n",
    "\n",
    "* to do agglomerative clustering with sklearn ([sklearn.cluster.AgglomerativeClustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html))\n",
    "* to draw dendograms with sklearn ([doc](https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py))\n",
    "\n",
    "In the following example, we search for 3 clusters using a Euclidean distance and ward as a linkage criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29471770",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T18:11:56.121554Z",
     "start_time": "2021-11-13T18:11:55.781741Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "arrests_ac = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')  \n",
    "arrests_ac.fit_predict(scaled_df)\n",
    "\n",
    "plt.figure(figsize=(8.5,8.5))\n",
    "plt.xlim(-3.5,3.5)\n",
    "plt.ylim(-3.5,3.5)\n",
    "plt.xlabel(\"PC{}\".format(1))\n",
    "plt.ylabel(\"PC{}\".format(2))\n",
    "plt.grid()\n",
    "\n",
    "biplot(USArrests_pca, fitted_pca,\n",
    "       original_dim_labels=scaled_df.columns,\n",
    "       point_labels=USArrests['StateAbbrv'],\n",
    "       cluster=arrests_ac.labels_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
